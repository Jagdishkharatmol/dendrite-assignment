{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "One_More_Try.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import  train_test_split\n",
        "from sklearn.linear_model import LinearRegression,Lasso,LogisticRegression,SGDRegressor,Ridge\n",
        "from sklearn.tree import DecisionTreeClassifier,DecisionTreeRegressor\n",
        "from sklearn.model_selection import  train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor,RandomForestClassifier,GradientBoostingClassifier,GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error,accuracy_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "-JS-AJ-qNAr_"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder,PolynomialFeatures\n",
        "from sklearn.pipeline import Pipeline,make_pipeline\n",
        "from sklearn.feature_selection import SelectKBest,f_regression\n",
        "import json"
      ],
      "metadata": {
        "id": "YIirea1xNgwc"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Models:\n",
        "  def __init__(self,model_info_path='/content/algoparams_from_ui.json',datapath=\"/content/iris.csv\"):\n",
        "    self.df=pd.read_csv(datapath)\n",
        "    self.model_info=json.load(open(model_info_path))\n",
        "  \n",
        "  def feature_handling(self):\n",
        "    # imputation transformer\n",
        "    sepal_value=self.model_info['design_state_data'][\"feature_handling\"]['sepal_width']['feature_details']['impute_value']\n",
        "    petal_value=self.model_info['design_state_data'][\"feature_handling\"]['petal_width']['feature_details']['impute_value']\n",
        "    trf1 = ColumnTransformer([\n",
        "        ('impute_sepal_length',SimpleImputer(),[0]),\n",
        "        ('impute_sepal_width',SimpleImputer(strategy ='constant', fill_value=sepal_value),[1]),\n",
        "        ('impute_petal_length',SimpleImputer(),[2]),\n",
        "        ('impute_petal_width',SimpleImputer(strategy ='constant', fill_value=petal_value),[3])\n",
        "    ],remainder='passthrough')\n",
        "\n",
        "    # one hot encoding\n",
        "    trf2 = ColumnTransformer([\n",
        "        ('ohe_species',OneHotEncoder(sparse=False,handle_unknown='ignore'),[4])\n",
        "    ],remainder='passthrough')\n",
        "\n",
        "    pipe = Pipeline([\n",
        "    ('trf1',trf1),\n",
        "    ('trf2',trf2),\n",
        "    ])\n",
        "    d_pipe=pipe.fit_transform(self.df)\n",
        "    return d_pipe\n",
        "\n",
        "  def feature_generation(self):\n",
        "    X=pd.DataFrame(zip(self.df['petal_length']/self.df['sepal_width'],self.df['petal_width']/len(self.df['species'].unique())))\n",
        "    poly = PolynomialFeatures(interaction_only=True,include_bias = False)\n",
        "    self.df[[\"new_feature_3\",\"new_feature_4\",\"new_feature_5\"]]=poly.fit_transform(X)\n",
        "    self.df['new_feature_1']=self.df[\"petal_length\"]*self.df[\"sepal_width\"]\n",
        "    self.df['new_feature_2']=self.df[\"sepal_width\"]/self.df[\"sepal_length\"]\n",
        "    self.df['new_feature_3']=self.df[\"petal_width\"]/self.df[\"sepal_length\"]\n",
        "    return self.df\n",
        "\n",
        "  def trans_cvrt(self,d_pipe):\n",
        "      columns_all=['species_1','species_2','species_3','sepal_length','sepal_width','petal_length','petal_width','new_feature_3','new_feature_4','new_feature_5','new_feature_1','new_feature_2']\n",
        "      df=pd.DataFrame(data=d_pipe,columns=columns_all)\n",
        "      cnt_int=['species_1','species_2',\t'species_3',]\n",
        "      cnt_float=['sepal_length','sepal_width','petal_length','petal_width','new_feature_3','new_feature_4','new_feature_5','new_feature_1','new_feature_2']\n",
        "      for column in columns_all:\n",
        "        if column in cnt_int:\n",
        "          df[column]=df[column].astype('int')\n",
        "        elif column in cnt_float:\n",
        "          df[column]=df[column].astype('float')\n",
        "      \n",
        "      #creating the dependent and independent frames\n",
        "      x=df.drop([model_info['design_state_data']['target']['target']],axis=1)\n",
        "      y=df[model_info['design_state_data']['target']['target']]\n",
        "      return x,y\n",
        "\n",
        "  def feature_reduction(self,x,y):\n",
        "    #Select top  features based on mutual info regression\n",
        "    selector = SelectKBest(f_regression, k=int(self.model_info['design_state_data'][\"feature_reduction\"]['num_of_features_to_keep']))\n",
        "    selector.fit(x, y)\n",
        "    x_names=list(x.columns[selector.get_support()])\n",
        "    x=x[x_names]\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "  def model_objects():\n",
        "        rr=Ridge()\n",
        "        logr=LogisticRegression()\n",
        "        lr=LinearRegression()\n",
        "        gbr_model=GradientBoostingRegressor()\n",
        "        gbc_model=GradientBoostingClassifier()\n",
        "        rfc_model=RandomForestClassifier()\n",
        "        rfr_model=RandomForestRegressor()\n",
        "        sgd=SGDRegressor()\n",
        "        dtc=DecisionTreeClassifier()\n",
        "        dtr=DecisionTreeRegressor()\n",
        "        la=Lasso()\n",
        "        return rr,logr,lr,gbr_model,gbc_model,rfc_model,rfr_model,sgd,dtc,dtr,la\n",
        "  \n",
        "\n",
        "  def splits(self,x,y):\n",
        "    xtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.2,shuffle=True)\n",
        "    return xtrain,xtest,ytrain,ytest\n",
        "\n",
        "  def execution(self):\n",
        "    self.df=Models.feature_generation(self)\n",
        "    d_pipe=Models.feature_handling(self)\n",
        "    x,y=Models.trans_cvrt(self,d_pipe)\n",
        "    x=Models.feature_reduction(self,x,y)\n",
        "    xtrain,xtest,ytrain,ytest=Models.splits(self,x,y)\n",
        "    return xtrain,xtest,ytrain,ytest\n",
        "\n",
        "  def model_prediction(self):\n",
        "    xtrain,xtest,ytrain,ytest=Models.execution(self)\n",
        "    rr,logr,lr,gbr_model,gbc_model,rfc_model,rfr_model,sgd,dtc,dtr,la=Models.model_objects()\n",
        "    model_info=self.model_info\n",
        "    #finding out which model to run\n",
        "    model_instance={}\n",
        "    for name in model_info['design_state_data']['algorithms'].keys():\n",
        "      model_instance[name]=model_info['design_state_data']['algorithms'][name]['is_selected']\n",
        "\n",
        "    if model_instance['RandomForestRegressor']:\n",
        "        param_grid={'n_estimators':np.arange(model_info['design_state_data']['algorithms']['RandomForestRegressor'][\"min_trees\"],model_info['design_state_data']['algorithms']['RandomForestRegressor'][\"max_trees\"]+1),\n",
        "                    'max_depth':np.arange(model_info['design_state_data']['algorithms']['RandomForestRegressor'][\"min_depth\"],model_info['design_state_data']['algorithms']['RandomForestRegressor'][\"max_depth\"]+1),\n",
        "                    'min_samples_leaf':np.arange(model_info['design_state_data']['algorithms']['RandomForestRegressor'][\"min_samples_per_leaf_min_value\"],model_info['design_state_data']['algorithms']['RandomForestRegressor'][\"min_samples_per_leaf_max_value\"]+1)}\n",
        "\n",
        "        cv_rf_model=GridSearchCV(rfr_model, param_grid, scoring=None, n_jobs=model_info['design_state_data'][\"hyperparameters\"][\"parallelism\"], refit=True, \n",
        "                                cv=model_info['design_state_data'][\"hyperparameters\"][\"num_of_folds\"],\n",
        "                                verbose=0, pre_dispatch='2*n_jobs', error_score=np.nan, return_train_score=False)\n",
        "\n",
        "        cv_rf_model.fit(xtrain,ytrain)\n",
        "        pred_y=cv_rf_model.predict(xtest)\n",
        "        print(f\"Random Forest Regressor  mean squared error is {round(mean_squared_error(ytest,pred_y),4)}\")  \n",
        "    \n",
        "    elif model_instance['RandomForestClassifier']:\n",
        "        param_grid={'n_estimators':np.arange(model_info['design_state_data']['algorithms']['RandomForestClassifier'][\"min_trees\"],model_info['design_state_data']['algorithms']['RandomForestClassifier'][\"max_trees\"]+1),\n",
        "                    'max_depth':np.arange(model_info['design_state_data']['algorithms']['RandomForestClassifier'][\"min_depth\"],model_info['design_state_data']['algorithms']['RandomForestClassifier'][\"max_depth\"]+1),\n",
        "                    'min_samples_leaf':np.arange(model_info['design_state_data']['algorithms']['RandomForestClassifier'][\"min_samples_per_leaf_min_value\"],model_info['design_state_data']['algorithms']['RandomForestClassifier'][\"min_samples_per_leaf_max_value\"]+1)}\n",
        "\n",
        "        cv_rf_model=GridSearchCV(rfc_model, param_grid, scoring=None, n_jobs=model_info['design_state_data'][\"hyperparameters\"][\"parallelism\"], refit=True, \n",
        "                                cv=model_info['design_state_data'][\"hyperparameters\"][\"num_of_folds\"],\n",
        "                                verbose=0, pre_dispatch='2*n_jobs', error_score=np.nan, return_train_score=False)\n",
        "\n",
        "\n",
        "        cv_rf_model.fit(xtrain,ytrain)\n",
        "        pred_y=cv_rf_model.predict(xtest)\n",
        "        print(f\"Random Forest classifier  accuracy is {accuracy_score(ytest,pred_y)}\")\n",
        "\n",
        "    elif model_instance['GBTClassifier']:\n",
        "      param_grid={'subsample':np.arange(model_info['design_state_data']['algorithms']['GBTClassifier'][\"min_subsample\"],model_info['design_state_data']['algorithms']['GBTClassifier'][\"max_subsample\"]+1),\n",
        "                  'max_depth':np.arange(model_info['design_state_data']['algorithms']['GBTClassifier'][\"min_depth\"],model_info['design_state_data']['algorithms']['GBTClassifier'][\"max_depth\"]+1),\n",
        "                  'n_iter_no_change':np.arange(model_info['design_state_data']['algorithms']['GBTClassifier'][\"min_iter\"],model_info['design_state_data']['algorithms']['GBTClassifier'][\"max_iter\"]+1)}\n",
        "\n",
        "      cv_gb_model=GridSearchCV(gbc_model, param_grid, scoring=None, n_jobs=model_info['design_state_data'][\"hyperparameters\"][\"parallelism\"], refit=True, \n",
        "                              cv=model_info['design_state_data'][\"hyperparameters\"][\"num_of_folds\"],\n",
        "                              verbose=0, pre_dispatch='2*n_jobs', error_score=np.nan, return_train_score=False)\n",
        "\n",
        "      cv_gb_model.fit(xtrain,ytrain)\n",
        "      pred_y=cv_gb_model.predict(xtest)\n",
        "      print(f\"Gradient Boosted classifier  accuracy is {accuracy_score(ytest,pred_y)}\")\n",
        "\n",
        "\n",
        "    elif model_instance['GBTRegressor']:\n",
        "      param_grid={'subsample':np.arange(model_info['design_state_data']['algorithms']['GBTRegressor'][\"min_subsample\"],model_info['design_state_data']['algorithms']['GBTRegressor'][\"max_subsample\"]+1),\n",
        "                  'max_depth':np.arange(model_info['design_state_data']['algorithms']['GBTRegressor'][\"min_depth\"],model_info['design_state_data']['algorithms']['GBTRegressor'][\"max_depth\"]+1),\n",
        "                  'n_iter_no_change':np.arange(model_info['design_state_data']['algorithms']['GBTRegressor'][\"min_iter\"],model_info['design_state_data']['algorithms']['GBTRegressor'][\"max_iter\"]+1)}\n",
        "\n",
        "      cv_gb_model=GridSearchCV(gbr_model, param_grid, scoring=None, n_jobs=model_info['design_state_data'][\"hyperparameters\"][\"parallelism\"], refit=True, \n",
        "                              cv=model_info['design_state_data'][\"hyperparameters\"][\"num_of_folds\"],\n",
        "                              verbose=0, pre_dispatch='2*n_jobs', error_score=np.nan, return_train_score=False)\n",
        "\n",
        "      cv_gb_model.fit(xtrain,ytrain)\n",
        "      pred_y=cv_gb_model.predict(xtest)\n",
        "      print(f\"Gradient Boosted Regressor  mean squared error is {round(mean_squared_error(ytest,pred_y),2)}\")\n",
        "\n",
        "    elif model_instance['LinearRegression']:\n",
        "      lr.fit(xtrain,ytrain)\n",
        "      pred_y=lr.predict(xtest)\n",
        "      print(f\"Linear Regression  mean squared error is {round(mean_squared_error(ytest,pred_y),2)}\")\n",
        "\n",
        "    elif model_instance['LogisticRegression']:\n",
        "      param_grid={'max_iter':np.arange(model_info['design_state_data']['algorithms']['LogisticRegression'][\"min_iter\"],model_info['design_state_data']['algorithms']['LogisticRegression'][\"max_iter\"]+1),\n",
        "                  }\n",
        "\n",
        "      cv_logr_model=GridSearchCV(logr, param_grid, scoring=None, n_jobs=model_info['design_state_data'][\"hyperparameters\"][\"parallelism\"], refit=True, \n",
        "                              cv=model_info['design_state_data'][\"hyperparameters\"][\"num_of_folds\"],\n",
        "                              verbose=0, pre_dispatch='2*n_jobs', error_score=np.nan, return_train_score=False)\n",
        "\n",
        "      cv_logr_model.fit(xtrain,ytrain)\n",
        "      pred_y=cv_logr_model.predict(xtest)\n",
        "      print(f\"Logistic Regression  accuracy is {accuracy_score(ytest,pred_y)}\")\n",
        "\n",
        "    elif model_instance['RidgeRegression']:\n",
        "      param_grid={'max_iter':np.arange(model_info['design_state_data']['algorithms'][\"RidgeRegression\"][\"min_iter\"],model_info['design_state_data']['algorithms'][\"RidgeRegression\"][\"max_iter\"]+1),\n",
        "                  }\n",
        "\n",
        "      cv_rr_model=GridSearchCV(rr, param_grid, scoring=None, n_jobs=model_info['design_state_data'][\"hyperparameters\"][\"parallelism\"], refit=True, \n",
        "                              cv=model_info['design_state_data'][\"hyperparameters\"][\"num_of_folds\"],\n",
        "                              verbose=0, pre_dispatch='2*n_jobs', error_score=np.nan, return_train_score=False)\n",
        "\n",
        "      cv_rr_model.fit(xtrain,ytrain)\n",
        "      pred_y=cv_rr_model.predict(xtest)\n",
        "\n",
        "      print(f\"Ridge Regressor  mean squared error is {round(mean_squared_error(ytest,pred_y),2)}\")\n",
        "\n",
        "    elif model_instance['LassoRegression']:\n",
        "      param_grid={'max_iter':np.arange(model_info['design_state_data']['algorithms'][\"LassoRegression\"][\"min_iter\"],model_info['design_state_data']['algorithms'][\"LassoRegression\"][\"max_iter\"]+1),}\n",
        "      cv_lr_model=GridSearchCV(la, param_grid, scoring=None, n_jobs=model_info['design_state_data'][\"hyperparameters\"][\"parallelism\"], refit=True, \n",
        "                              cv=model_info['design_state_data'][\"hyperparameters\"][\"num_of_folds\"],\n",
        "                              verbose=0, pre_dispatch='2*n_jobs', error_score=np.nan, return_train_score=False)\n",
        "\n",
        "      cv_lr_model.fit(xtrain,ytrain)\n",
        "      pred_y=cv_lr_model.predict(xtest)\n",
        "      print(f\"Lasso Regressor  mean squared error is {round(mean_squared_error(ytest,pred_y),2)}\")\n",
        "\n",
        "    elif model_instance['DecisionTreeRegressor']:\n",
        "      param_grid={'max_depth':np.arange(model_info['design_state_data']['algorithms'][\"DecisionTreeRegressor\"][\"min_depth\"],model_info['design_state_data']['algorithms'][\"DecisionTreeRegressor\"][\"max_depth\"]+1)}\n",
        "\n",
        "      cv_dtr_model=GridSearchCV(dtr, param_grid, scoring=None, n_jobs=model_info['design_state_data'][\"hyperparameters\"][\"parallelism\"], refit=True, \n",
        "                              cv=model_info['design_state_data'][\"hyperparameters\"][\"num_of_folds\"],\n",
        "                              verbose=0, pre_dispatch='2*n_jobs', error_score=np.nan, return_train_score=False)\n",
        "\n",
        "      cv_dtr_model.fit(xtrain,ytrain)\n",
        "      pred_y=cv_dtr_model.predict(xtest)\n",
        "      print(f\"Decision Tree Regression  mean square error is {round(mean_squared_error(ytest,pred_y),2)}\")\n",
        "\n",
        "    elif model_instance['DecisionTreeClassifier']:\n",
        "      if model_info['design_state_data']['algorithms'][\"DecisionTreeClassifier\"][\"use_gini\"]==False:\n",
        "        criterion='entropy'\n",
        "      else:\n",
        "        criterion='gini'\n",
        "\n",
        "      param_grid={'max_depth':np.arange(model_info['design_state_data']['algorithms'][\"DecisionTreeClassifier\"][\"min_depth\"],model_info['design_state_data']['algorithms'][\"DecisionTreeClassifier\"][\"max_depth\"]+1)}\n",
        "\n",
        "      cv_dtc_model=GridSearchCV(dtc, param_grid, scoring=None, n_jobs=model_info['design_state_data'][\"hyperparameters\"][\"parallelism\"], refit=True, \n",
        "                              cv=model_info['design_state_data'][\"hyperparameters\"][\"num_of_folds\"],\n",
        "                              verbose=0, pre_dispatch='2*n_jobs', error_score=np.nan, return_train_score=False)\n",
        "\n",
        "      cv_dtc_model.fit(xtrain,ytrain)\n",
        "      pred_y=cv_dtc_model.predict(xtest)\n",
        "      print(f\"Decision Tree classifier  accuracy is {accuracy_score(ytest,pred_y)}\")\n",
        "\n",
        "    return 1\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model_instance = Models()\n",
        "    model_instance.model_prediction()\n",
        "  "
      ],
      "metadata": {
        "id": "IMoqkmNRN7cW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30f6531b-9520-4369-b384-ae83c97e7aa6"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Regressor  mean squared error is 0.0026\n"
          ]
        }
      ]
    }
  ]
}